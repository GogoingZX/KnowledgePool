knitr::opts_chunk$set(echo = TRUE)
# importing GPA data and saving as gpa.example
gpa.example = read.table("/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/CH01PR19.txt")
# assinging names
colnames(gpa.example) = c("Y", "X")
# viewing the data
#head(gpa.example)
#tail(gpa.example)
# accessing the data
## accessing the element in the 2nd row, 1st column
##gpa.example[2,1]
## accessing the 100th row
##gpa.example[100,]
## accessing the 1st column
##gpa.example[,1]
# running linear regression
lm.gpa = lm(Y~X, data=gpa.example)
lm.gpa
summary(lm.gpa)
## b_1
b_1_up = sum((gpa.example$X-mean(gpa.example$X))*(gpa.example$Y-mean(gpa.example$Y)))
b_1_down = sum((gpa.example$X-mean(gpa.example$X))^2)
b_1 = b_1_up/b_1_down
b_1 # 0.03882713
## b_0
b_0 = mean(gpa.example$Y)-b_1*mean(gpa.example$X)
b_0 # 2.114049
## s
dim(gpa.example) # 120   2
s2 = sum((gpa.example$Y-(b_0+b_1*gpa.example$X))^2)/(120-2)
s = sqrt(s2)
s # 0.623125
## s{b_1}
s_b_1 = sqrt(s2/b_1_down)
s_b_1 # 0.01277302
# constructing 95% confidence interval
## 0.975 quantile of t-dist, with d.f n-2=118
qt(1-0.05/2, df=118) # 1.980272
width = qt(1-0.05/2, df=118)*s_b_1
c(b_1-width, b_1+width) # 0.01353307 0.06412118
## confidence interval
confint(lm.gpa, level=0.95)
# constructing confidence interval for E[Y_h] at X=27 and X=32
newobs = data.frame(Y=c(NA), X=c(27,32))
pre_CI = predict(lm.gpa, newdata=newobs, interval="confidence", level=0.95)
pre_CI
# constructing prediction interval for E[Y_h] at X=27 and X=32
pre_PI = predict(lm.gpa, newdata=newobs, interval="prediction")
pre_PI
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
View(gpa.example)
# importing GPA data and saving as gpa.example
gpa.example = read.table("/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/CH01PR19.txt")
# assinging names
colnames(gpa.example) = c("Y", "X")
# viewing the data
head(gpa.example)
tail(gpa.example)
# importing GPA data and saving as gpa.example
gpa.example = read.table("/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/CH01PR19.txt")
summary(gpa.example)
# assinging names
colnames(gpa.example) = c("Y", "X")
# viewing the data
head(gpa.example)
tail(gpa.example)
# importing GPA data and saving as gpa.example
gpa.example = read.table("/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/CH01PR19.txt")
summary(gpa.example)
# assinging names
colnames(gpa.example) = c("Y", "X")
# viewing the data
head(gpa.example)
tail(gpa.example)
# accessing the data
# accessing the element in the 2nd row, 1st column
gpa.example[2,1]
## accessing the 100th row
gpa.example[100,]
## accessing the 1st column
gpa.example[,1]
?lm
# running linear regression
lm.gpa = lm(Y~X, data=gpa.example)
lm.gpa
summary(lm.gpa)
## b_1
b_1_up = sum((gpa.example$X-mean(gpa.example$X))*(gpa.example$Y-mean(gpa.example$Y)))
b_1_down = sum((gpa.example$X-mean(gpa.example$X))^2)
b_1 = b_1_up/b_1_down # b_1=\frac{\sum{(X_i-\bar{X})(Y_i-\bar{Y})}{\sum{(X_i-\bar{X})^2}}
b_1 # 0.03882713
## b_0
b_0 = mean(gpa.example$Y)-b_1*mean(gpa.example$X)
b_0 # 2.114049
## s
dim(gpa.example) # 120   2
s2 = sum((gpa.example$Y-(b_0+b_1*gpa.example$X))^2)/(120-2)
s = sqrt(s2)
s # 0.623125
## s{b_1}
s_b_1 = sqrt(s2/b_1_down)
s_b_1 # 0.01277302
# constructing 95% confidence interval
## 0.975 quantile of t-dist, with d.f n-2=118
qt(1-0.05/2, df=118) # 1.980272
width = qt(1-0.05/2, df=118)*s_b_1
c(b_1-width, b_1+width) # 0.01353307 0.06412118
## confidence interval
confint(lm.gpa, level=0.95)
# constructing confidence interval for E[Y_h] at X=27 and X=32
newobs = data.frame(Y=c(NA), X=c(27,32))
pre_CI = predict(lm.gpa, newdata=newobs, interval="confidence", level=0.95)
pre_CI
# constructing prediction interval for E[Y_h] at X=27 and X=32
pre_PI = predict(lm.gpa, newdata=newobs, interval="prediction")
pre_PI
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
dim(gpa.example)
?dim
## b_1
b_1_up = sum((gpa.example$X-mean(gpa.example$X))*(gpa.example$Y-mean(gpa.example$Y)))
b_1_down = sum((gpa.example$X-mean(gpa.example$X))^2)
b_1 = b_1_up/b_1_down # b_1=\frac{\sum{(X_i-\bar{X})(Y_i-\bar{Y})}{\sum{(X_i-\bar{X})^2}}
b_1 # 0.03882713
## b_0
b_0 = mean(gpa.example$Y)-b_1*mean(gpa.example$X)
b_0 # 2.114049
## s
dim(gpa.example) # 120   2
s2 = sum((gpa.example$Y-(b_0+b_1*gpa.example$X))^2)/(120-2)
s = sqrt(s2)
s # 0.623125
## s{b_1}
s_b_1 = sqrt(s2/b_1_down)
s_b_1 # 0.01277302
# constructing 95% confidence interval
## 0.975 quantile of t-dist, with d.f n-2=118
qt(1-0.05/2, df=118) # 1.980272
width = qt(1-0.05/2, df=118)*s_b_1
c(b_1-width, b_1+width) # 0.01353307 0.06412118
## confidence interval
confint(lm.gpa, level=0.95)
# constructing confidence interval for E[Y_h] at X=27 and X=32
newobs = data.frame(Y=c(NA), X=c(27,32))
pre_CI = predict(lm.gpa, newdata=newobs, interval="confidence", level=0.95)
pre_CI
# constructing prediction interval for E[Y_h] at X=27 and X=32
pre_PI = predict(lm.gpa, newdata=newobs, interval="prediction")
pre_PI
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
# constructing 95% confidence interval
## 0.975 quantile of t-dist, with d.f n-2=118
qt(1-0.05/2, df=118) # 1.980272
width = qt(1-0.05/2, df=118)*s_b_1
c(b_1-width, b_1+width) # 0.01353307 0.06412118
## confidence interval
confint(lm.gpa, level=0.95)
knitr::opts_chunk$set(echo = TRUE)
# constructing confidence interval for E[Y_h] at X=27 and X=32
newobs = data.frame(Y=c(NA), X=c(27,32))
pre_CI = predict(lm.gpa, newdata=newobs, interval="confidence", level=0.95)
pre_CI
# constructing prediction interval for E[Y_h] at X=27 and X=32
pre_PI = predict(lm.gpa, newdata=newobs, interval="prediction")
pre_PI
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
# constructing confidence interval for E[Y_h] at X=27 and X=32
newobs = data.frame(Y=c(NA), X=c(27,32))
pre_CI = predict(lm.gpa, newdata=newobs, interval="confidence", level=0.95)
pre_CI
# constructing prediction interval for E[Y_h] at X=27 and X=32
pre_PI = predict(lm.gpa, newdata=newobs, interval="prediction")
pre_PI
# constructing prediction interval for E[Y_h] at X=27 and X=32
pre_PI = predict(lm.gpa, newdata=newobs, interval="prediction", level=0.95)
pre_PI
# constructing prediction interval for E[Y_h] at X=27 and X=32
pre_PI = predict(lm.gpa, newdata=newobs, interval="prediction", level=0.95)
pre_PI
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
?aov
?anove
?anova
full.model = lm(Y~factor(X), data=gpa.example)
reduced.model = lm(Y~X, data=gpa.example)
f.r.anova(reduced.model, full.model)
full.model = lm(Y~factor(X), data=gpa.example)
reduced.model = lm(Y~X, data=gpa.example)
f.r.anova = anova(reduced.model, full.model)
full.model = lm(Y~factor(X), data=gpa.example)
reduced.model = lm(Y~X, data=gpa.example)
f.r.anova = anova(reduced.model, full.model)
f.r.anova
full.model = lm(Y~factor(X), data=gpa.example)
reduced.model = lm(Y~X, data=gpa.example)
f.r.anova = anova(full.model, reduced.model)
f.r.anova
full.model = lm(Y~factor(X), data=gpa.example)
reduced.model = lm(Y~X, data=gpa.example)
f.r.anova = anova(full.model, reduced.model)
f.r.anova
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
?aov
?anova
aov.gpa = anova(lm.gpa)
summary(aov.gpa)
aov.gpa = anova(lm.gpa)
summary(aov.gpa)
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
aov.gpa = anova(lm.gpa)
summary(aov.gpa)
aov.gpa = aov(lm.gpa)
summary(aov.gpa)
knitr::opts_chunk$set(echo = TRUE)
data(cars)
plot(cars)
View(cars)
data(cars)
plot(cars)
abline(lm(cars$dist~cars$speed), col="red")
title(main="dist~speed regression")
data(cars)
plot(cars)
r = lm(cars$dist~cars$speed)
abline(r, col="red")
data(cars)
plot(cars)
r = lm(cars$dist~cars$speed)
abline(r, col="red")
r1 = lm(cars$speed~cars$dist)
a = r$coefficients[1] # Intercept
b = r$coefficients[2] # slop
abline(-a/b, 1/b, col="blue")
title(main="dist~speed and speed~dist regression")
data(cars)
plot(cars)
r = lm(cars$dist~cars$speed)
abline(r, col="red")
r1 = lm(cars$speed~cars$dist)
a = r1$coefficients[1] # Intercept
b = r1$coefficients[2] # slop
abline(-a/b, 1/b, col="blue")
title(main="dist~speed and speed~dist regression")
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
a3 = r$center[2]-b*r$center[1]
b3 = r3$loadings[2,1]/r3$loadings[1,1]
abline(a3,b3, col="green")
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
a3 = r$center[2]-b*r$center[1]
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
b3 = r$center[2]-b*r$center[1]
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
a3 = r$center[2]-b*r3$center[1]
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
a3 = r3$center[2]-b*r3$center[1]
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
b3 = r3$loadings[2,1]/r3$loadings[1,1]
a3 = r3$center[2]-b3*r3$center[1]
abline(a3,b3, col="green")
title(main="Comparing three regressions")
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
b3 = r3$loadings[2,1]/r3$loadings[1,1]
a3 = r3$center[2]-b3*r3$center[1]
abline(a3,b3, col="green")
title(main="Comparing three regressions")
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
b3 = r3$loadings[2,1]/r3$loadings[1,1]
a3 = r3$center[2]-b3*r3$center[1]
abline(a3,b3, col="green")
title(main="Comparing three regressions")
data(cars)
plot(cars)
r1 = lm(cars$dist~cars$speed)
abline(r1, col="red")
r2 = lm(cars$speed~cars$dist)
a2 = r2$coefficients[1] # Intercept
b2 = r2$coefficients[2] # slop
abline(-a2/b2, 1/b2, col="blue")
r3 = princomp(cars)
b3 = r3$loadings[2,1]/r3$loadings[1,1]
a3 = r3$center[2]-b3*r3$center[1]
abline(a3,b3)
title(main="Comparing three regressions")
knitr::opts_chunk$set(echo = TRUE)
data_1 = read.csv(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/copier_maintenance.txt',
sep='', header=F)
data_1
y = data_1[,1]
x1 = data_1[,2]
x2 = data_1[,3]
data_1 = read.csv(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/copier_maintenance.txt',sep='', header=F)
data_1
y = data_1[,1]
x1 = data_1[,2]
x2 = data_1[,3]
data_1 = read.csv(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/copier_maintenance.txt',sep='', header=F)
data_1
y = data_1[,1]
x1 = data_1[,2]
x2 = data_1[,3]
data_1 = read.csv(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/grocery_retailer.txt',sep='', header=F)
data_1
y = data_1[,1]
x1 = data_1[,2]
x2 = data_1[,3]
x3 = data_1[,4]
fit = lm(y~x1+x2+x3, data=data_1)
# fit = lm(v1~v2+v3+v4, data=data_1)
summary(fit)
knitr::opts_chunk$set(echo = TRUE)
job = read.table(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/job_proficiency.txt, quote='\'', comment.char='')
job = read.table(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/job_proficiency.txt)
job = read.table(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/job_proficiency.txt')
anova(fmfull, fm)
job = read.table(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/job_proficiency.txt')
colnames(job) = c('y', 'x1', 'x2', 'x3', 'x4')
fm = lm(log(y)~., job)
fmfull = lm(log(y)~1, job)
anova(fmfull, fm)
fm13 = lm(log(y)~x1+x2+l(x1^2)+l(x2^2)+X1:x2, job)
fm13 = lm(log(y)~x1+x2+i(x1^2)+i(x2^2)+X1:x2, job)
fm13 = lm(log(y)~x1+x2+I(x1^2)+I(x2^2)+X1:x2, job)
job = read.table(
'/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/job_proficiency.txt')
colnames(job) = c('y', 'x1', 'x2', 'x3', 'x4')
fm = lm(log(y)~., job)
fmfull = lm(log(y)~1, job)
anova(fmfull, fm)
fm13 = lm(log(y)~x1+x2+I(x1^2)+I(x2^2)+X1:x2, job)
fm13 = lm(log(y)~x1+x2+I(x1^2)+I(x2^2)+x1:x2, job)
fm13 = lm(log(y)~x1+x2+I(x1^2)+I(x2^2)+x1:x2, job)
step(fm13, scope=list(lower=~1, upper=~x1+x2+I(x1^2)+I(x2^2)+x1:x2), direction='backward)
fm13 = lm(log(y)~x1+x2+I(x1^2)+I(x2^2)+x1:x2, job)
step(fm13, scope=list(lower=~1,upper=~x1+x2+I(x1^2)+I(x2^2)+x1:x2), direction='backward)
fm13 = lm(log(y)~x1+x2+I(x1^2)+I(x2^2)+x1:x2, job)
step(fm13,
scope=list(lower=~1,upper=~x1+x2+I(x1^2)+I(x2^2)+x1:x2), direction='backward')
data = read.table('/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/annual_dues.txt')
colnames(data)=c('y', 'x')
logit_fm = glm(y~x, data, family=bonomial('logit'))
data = read.table('/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/annual_dues.txt')
colnames(data)=c('y', 'x')
logit_fm = glm(y~x, data, family=binomial('logit'))
summary(logit_fm)
