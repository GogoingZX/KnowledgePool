---
title: "Tut3_ST5202"
author:
- Name:Zhu Xu
- User ID:E0337988
- Matriculation ID:A0191344H
date: "26/Mar/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\rule[-5pt]{16.3cm}{0.15em}
\linespread{1.3}

###Q1:

####a)

\begin{equation*}
\begin{split}
SSR(x_1,x_2,\cdots ,x_{p-1})&=SSR(x_1)+SSR(x_2|x_1)+SSR(x_3|x_1,x_2)+\cdots +SSR(x_{p-1}|x_1,\cdots,x_{p-2})\\
&=SSR(x_1,x_2)+SSR(x_3|x_1,x_2)+\cdots +SSR(X_{p-1}|x_1,\cdots ,x_{p-2})\\
&=SSR(x_1,x_2,x_3)+SSR(x_4|x_1,x_2,x_3)+\cdots +SSR(x_{p-1}|x_1,\cdots,x_{p-2})\\
&=\cdots\\
&=SSR(x_1,x_2,\cdots,x_{p-1})
\end{split}
\end{equation*}

####b)

Similarly,
\begin{equation*}
\begin{split}
SSR &=SSR(x_1, x_2,\cdots, x_{p-1})\\
&=SSR(x_1,x_2,x_3)+SSR(x_4|x_1,x_2,x_3)+\cdots +SSR(x_{p-1}|x_1,\cdots ,x_{p-2})\\
&=\cdots \\
&=SSR(x_p)+SSR(x_{p-1}|x_p)+SSR(x_{p-2}|x_{p-1},x_p)+\cdots+SSR(x_1|x_p,\cdots,x_2)
\end{split}
\end{equation*}

###5.3

$$Y=X\beta+\varepsilon\rightarrow\quad\varepsilon=Y-X\beta$$
Then minimize,

$$
\varepsilon^T\varepsilon=(Y-X\beta)^T(Y-X\beta)
$$
$$
2X^TX\beta-2X^TY=0
$$
$$
\hat{\beta}=(X^TX)^{-1}XY
$$
$$
\left[
\begin{matrix}
e_1\\
e_2\\
e_3\\
e_4\\
\end{matrix}
\right]
=
\left[
\begin{matrix}
Y_1\\
Y_2\\
Y_3\\
Y_4\\
\end{matrix}
\right]
-
\left[
\begin{matrix}
\hat{Y_1}\\
\hat{Y_2}\\
\hat{Y_3}\\
\hat{Y_4}\\
\end{matrix}
\right],
\qquad
X_{ij}
\left[
\begin{matrix}
e_1\\
e_2\\
e_3\\
e_4\\
\end{matrix}
\right]
=
\left[
\begin{matrix}
0 & 0 & \cdots & 0
\end{matrix}
\right]_{p\times 1}
$$

\newpage
###5.17

####a)

$$
\left[
\begin{matrix}
W_1\\
W_2\\
W_3\\
\end{matrix}
\right]
=
\left[
\begin{matrix}
1 & 1 & 1 \\
1 & -1 & 0 \\
1 & -1 & -1 \\
\end{matrix}
\right]
\left[
\begin{matrix}
Y_1\\
Y_2\\
Y_3\\
\end{matrix}
\right]
$$

####b)

$$
E(W)=
\left[
\begin{matrix}
E(Y_1)+E(Y_2)+E(Y_3)\\
E(Y_1)-E(Y_2)\\
E(Y_1)-E(Y_2)-E(Y_3)\\
\end{matrix}
\right]
$$

####c)

$$
cov(W)=
\left[
\begin{matrix}
Var(Y_1)+Var(Y_2)+Var(Y_3) & Var(Y_1)-Var(Y_2) & Var(Y_1)-Var(Y_2)-Var(Y_3) \\
Var(Y_1)-Var(Y_2) & Var(Y_1)+Var(Y_2) & Var(Y_1)+Var(Y_2) \\
Var(Y_1)-Var(Y_2)-Var(Y_3) & Var(Y_1)+Var(Y_2) & Var(Y_1)+Var(Y_2)+Var(Y_3) \\
\end{matrix}
\right]
$$

###6.22

####a)

It's a general linear regression model.

####b)

No, $logY_i=\beta_0+\beta_1X_{i1}+\beta_2X_{i2}^2+\varepsilon$

####c)

No, $Y_i=log\beta_1+logX_{i1}+\beta_2X_{i2}+\varepsilon$

####d)

No, it has no transformation

####e)

No, $log(\frac{1}{Y_i}-1)=\beta_0+\beta_1X_{i1}\varepsilon$

\newpage
###6.10

#### a)
```{r}
data_1 = read.csv(
  '/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/grocery_retailer.txt',
  sep='', header=F)
y = data_1[,1]
x1 = data_1[,2]
x2 = data_1[,3]
x3 = data_1[,4]
fit = lm(y~x1+x2+x3, data=data_1)
# fit = lm(v1~v2+v3+v4, data=data_1)
summary(fit)
```

\newpage
#### b)
```{r}
error = y - 4.150e+03 - 7.871e-04*x1 + 1.317e+01*x2 - 6.236e+02*x3
plot(error)
```

### 6.12
```{r Warnings=FALSE, message=FALSE}
new_x1 = c(302000, 245000, 280000, 350000, 295000)
new_x2 = c(7.20, 7.40, 6.90, 7.00, 6.70)
new_x3 = c(0, 0, 0, 0, 1)
new_data_1 = data.frame(new_x1, new_x2, new_x3)
predict_result = predict(fit, newdata=new_data_1, interval='confidence')
predict_result = data.frame(predict_result)

tvalue = qt(1-0.05/2, 50-4)
tvalue

bond = qt(1-0.05/(2*5), 50-4)
bond

width = sqrt(2*qf(1-0.05, 4, 50-4))
width

calfun_lwr = function(a){
  v = (predict_result$lwr - predict_result$fit)/tvalue * a + predict_result$fit
  return(v)
}

calfun_upr = function(b){
  v = (predict_result$lwr - predict_result$fit)/tvalue * b + predict_result$fit
  return(v)
}

bond_lwr = calfun_lwr(bond)
bond_upr = calfun_upr(bond)
width_lwr = calfun_lwr(width)
width_upr = calfun_upr(width)

CI = data.frame(bond_lwr, bond_upr, width_lwr, width_upr)
summary(CI)
```

\newpage
### 7.4

#### a)
```{r}
new_fit = lm(y~x1+x3+x2, data=data_1)
anova(new_fit)
```

####b)

$$
F=\frac{SSR(X2|X1,X3)}{\frac{SSR(X1)+SSR(X3|X1)}{50-3}}=0.1445783
$$
```{r}
F = 0.1445
if(F < qf(1-0.05,1, 47)){
	print("X2 can be dropped")
}
```

#### c)

They are equal.

### 7.25

#### a)
```{r}
new_fit_1 = lm(y~x1, data=data_1)
anova(new_fit_1)
```

\newpage
#### b)

The original $b_1$ is $7.871\times 10^{-4}$, the new one is $9.355\times 10^{-4}$

#### c)
```{r}
anova(fit)
new_fit_2 = lm(y~x1+x3+x2, data=data_1)
anova(new_fit_2)
```

