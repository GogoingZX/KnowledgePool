---
title: "ST5202_Tut4"
author: 
- Name:Zhu Xu
- User ID:E0337988
- Matriculation ID:A0191344H
date: "9/April/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\rule[-5pt]{16.3cm}{0.15em}
\linespread{1.3}

###8.15:

####a)

```{r}
copier = cbind(read.table(
  "/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/copier_maintenance.txt"),
  read.table(
    "/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/copier_maintenance_additional.txt"))
colnames(copier) = c("Y","X1","X2") 
fit.lm = lm(Y ~ X1 + X2, data=copier) 
summary(fit.lm)

```

$b_0=-0.9225$ is the intercept of the model, $b_1=15.0461$ means if one new copier is serviced the number of minutes will increase 15.0461, $b_2=0.7587$ means if the one of the copiers changes from large to small, the total time will increase 0.7587 minutes.

\newpage
####b)

The fitted model is 

$$
\hat{Y}=-0.9225+15.0461X_1+0.7587X_2
$$

####c)

```{r}
b_2 = summary(fit.lm)$coefficients[ 3, 1] 
sd_2 = summary(fit.lm)$coefficients[ 3, 2] 
paste0("(",round(b_2 - qt(0.975,42)*sd_2,2),
       ",",round(b_2 + qt(0.975,42)*sd_2,2),")")
```

The $95\%$ confidence interval is $(-4.85, 6.37)$.

####d)

Since if it is not concluded, the fitted result may be undervalued.

####e)

```{r}
par(mfrow=c(1,1))
plot(y=fit.lm$residuals,x=copier$X1*copier$X2,xlab="X1:X2",
                       ylab="Residuals") 
abline(0,0)
```

\newpage
###8.19:

####a)

```{r}
fit.lm1 <- lm(Y ~ X1 + X2 + X1:X2, data=copier)
summary(fit.lm1)
```

The fitted model is

$$
\hat(Y)=2.8131+14.3994X_1-8.1412X_2+1.7774X_1X_2
$$

\newpage
####b)

```{r}
anova(fit.lm,fit.lm1)
```

$H_0:E\{Y\}=\beta_0+\beta_1X_1+\beta_2X_2,\qquad H_1:E\{Y\}=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_1X_2$

The F-statistic is $3.326$, which is larger than $F(0.9, 41)$, thus we reject $H_0$

###Q1:

####a)

```{r}
 plot(y=copier$Y, x=copier$X1,
      xlab="X1", ylab="Y", 
      col=as.factor(copier$X2), pch=20) 
abline(coefficients(fit.lm)[1], coefficients(fit.lm)[2]) 
abline(coefficients(fit.lm)[1]+coefficients(fit.lm)[3], 
       coefficients(fit.lm)[2],col="red")
```

\newpage
####b)

```{r}
plot(y=copier$Y,x=copier$X1, 
     xlab="X1",ylab="Y", 
     col=as.factor(copier$X2),pch=20) 
abline(coefficients(fit.lm1)[1], coefficients(fit.lm1)[2]) 
abline(coefficients(fit.lm1)[1]+coefficients(fit.lm1)[3],
       coefficients(fit.lm1)[2]+coefficients(fit.lm1)[4],
       col="red")
```

###8.21:

####a)

For hard hat: $E\{Y\}=\beta_0+\beta_1X_1+\beta_2X_2$

For bump cap: $E\{Y\}=\beta_0+\beta_1X_1+\beta_3X_3$

For none: $E\{Y\}=\beta_0+\beta_1X_1$

####b)

(1) $H_0:\beta_3<0,\quad H_a:\beta_3\geq0$

(2) $H_0:\beta_2=\beta_3,\quad H_a:\beta_2\neq\beta_3$

\newpage
###9.10:

```{r}
jobdata = read.table(
  "/Users/xuzhu/Desktop/Notes/Sem2/ST5202-Applied_Regression_Analysis/Tut/job_proficiency.txt") 
colnames(jobdata) = c("X1",'X2','X3','X4','Y')
fit.lm <- lm(Y~.,data=jobdata)
summary(fit.lm)
```

$X_3$ and $X_4$ are not significant, we should not keep them.

\newpage
###9.11:

####a)

```{r}
library('MuMIn')
options(na.action = "na.fail")
combinations = dredge(fit.lm,extra="adjR^2")
print(combinations)
```

So the 4 best regression models are:
\begin{center}
$E\{Y\}_1=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4$

$E\{Y\}_2=\beta_0+\beta_1X_1+\beta_2X_2+\beta_4X_4$

$E\{Y\}_3=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3$

$E\{Y\}_2=\beta_0+\beta_1X_1+\beta_2X_2$
\end{center}
####b)

R-squard, AIC, BIC...

###Q2:

####a)

```{r}
combinations = dredge(fit.lm,rank='AIC')
print(combinations)
```

The best 4 models according to AIC are models with varibles(1,2,4), (1,2), (1), (1,2,3,4)

####b)

```{r}
combinations <- dredge(fit.lm,rank='BIC')
print(combinations)
```

Model with variables(1,2,4), (1), (1,2), (1,2,3,4) according to BIC.

###9.18:

```{r}
fit.lm = lm(Y ~ 1, data =jobdata) 
anova(lm(Y ~ X1, data =jobdata),fit.lm)
anova(lm(Y ~ X2, data =jobdata),fit.lm)
anova(lm(Y ~ X3, data =jobdata),fit.lm)
anova(lm(Y ~ X4, data =jobdata),fit.lm)
fit.lm =update(fit.lm, .~. +X1)
anova(fit.lm,lm(Y ~ X1+X2, data =jobdata))
anova(fit.lm,lm(Y ~ X1+X3, data =jobdata))
anova(fit.lm,lm(Y ~ X1+X4, data =jobdata))
```

The model shoud be $E\{Y\}=\beta_0+\beta_1X_1$

####b)

$$
E\{Y\}=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4
$$

###Q3:

####a)

$$
E(Y)=\beta_0+\beta_1X_1+\beta_3X_3+\beta_4X_4
$$

####b)

$$
E(Y)=\beta_0+\beta_1X_1+\beta_3X_3+\beta_4X_4
$$

####c)

Comparing AIC of simple linear regression model and simple population model, finding all 4 models can decrease the AIC of simple population model. We choose the minimum one as best one, thus we add $X_3, X_1, X_4$ into our model.

$$
E(Y)=\beta_0+\beta_1X_1+\beta_3X_3+\beta_4X_4
$$

####d)

We drop the variable $X_2$.

$$
E(Y)=\beta_0+\beta_1X_1+\beta_3X_3+\beta_4X_4
$$



